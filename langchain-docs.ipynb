{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.base import Document\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.utilities import ApifyWrapper\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"https://python.langchain.com/en/latest/\"}]},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141\n",
      "33\n",
      "1141\n",
      "6992\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ContextualCompressionRetriever\nbase_retriever\n  instance of BaseRetriever expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseRetriever)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m compressor \u001b[39m=\u001b[39m LLMChainExtractor\u001b[39m.\u001b[39mfrom_llm(llm)\n\u001b[0;32m---> 59\u001b[0m compression_retriever \u001b[39m=\u001b[39m ContextualCompressionRetriever(base_compressor\u001b[39m=\u001b[39;49mcompressor, base_retriever\u001b[39m=\u001b[39;49mretriever)\n\u001b[1;32m     61\u001b[0m compressed_docs \u001b[39m=\u001b[39m compression_retriever\u001b[39m.\u001b[39mget_relevant_documents(\u001b[39m\"\u001b[39m\u001b[39mWhat did the president say about Ketanji Jackson Brown\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m pretty_print_docs(compressed_docs)\n",
      "File \u001b[0;32m~/Documents/code/ai/langchain-fun/langchain-tutorials/.venv/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ContextualCompressionRetriever\nbase_retriever\n  instance of BaseRetriever expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseRetriever)"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "import os\n",
    "\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "cleaned = []\n",
    "\n",
    "common = \"Skip to main content \\nCtrl+K \\nü¶úüîó LangChain 0.0.152\\nBuild MongoDB Atlas databases with Python, Java, C# & more. Try it for free today.\\nAd by EthicalAds ¬∑ ‚ÑπÔ∏è\\nv: latest \\nVersions latest stable harrison/docs-refactor-3-24 \\nDownloads HTML \\nOn Read the Docs Project Home Builds Downloads \\nOn GitHub View Edit \\nSearch \\nHosted by Read the Docs ¬∑ Privacy Policy\"\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for doc in documents:\n",
    "    original_content = doc.page_content\n",
    "    doc.page_content = doc.page_content.replace(common, \"\")\n",
    "    if original_content != doc.page_content:\n",
    "        counter += 1\n",
    "    cleaned.append(doc)\n",
    "\n",
    "print(counter)\n",
    "\n",
    "documents = cleaned\n",
    "\n",
    "len_arr = [len(doc.page_content) for doc in documents]\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "len_arr2 = [len(doc.page_content) for doc in docs]\n",
    "\n",
    "print(len(len_arr))\n",
    "print(len(len_arr2))\n",
    "\n",
    "if len_arr == len_arr2:\n",
    "    print(\"Error\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "import pinecone \n",
    "\n",
    "# initialize pinecone\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone.init(\n",
    "    api_key=pinecone_api_key,\n",
    "    environment=\"us-east1-gcp\"  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"langchain-docs\"\n",
    "\n",
    "retriever = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"What did the president say about Ketanji Jackson Brown\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Conversational Agent with Tools (Langchain AGI)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "from langchain.agents import Tool\n",
      "from langchain.agents import AgentType\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain import OpenAI\n",
      "from langchain.utilities import SerpAPIWrapper\n",
      "from langchain.agents import initialize_agent\n",
      "search = SerpAPIWrapper()\n",
      "tools = [\n",
      "    Tool(\n",
      "        name = \"Current Search\",\n",
      "        func=search.run,\n",
      "        description=\"useful for when you need to answer questions about current events or the current state of the world\"\n",
      "    ),\n",
      "]\n",
      "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "llm=OpenAI(temperature=0)\n",
      "agent_chain = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "search = SerpAPIWrapper()\n",
      "search_tool = Tool(\n",
      "        name = \"Search\",\n",
      "        func=search.run,\n",
      "        description=\"useful for when you need to answer questions about current events\"\n",
      "    )\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.agents import load_tools, initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "\n",
      "llm = ChatOpenAI(temperature=0.0)\n",
      "math_llm = OpenAI(temperature=0.0)\n",
      "tools = load_tools(\n",
      "    [\"human\", \"llm-math\"], \n",
      "    llm=math_llm,\n",
      ")\n",
      "\n",
      "agent_chain = initialize_agent(\n",
      "    tools,\n",
      "    llm,\n",
      "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
      "    verbose=True,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "retriever2 = retriever.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever2)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"Give me a tool to create a conversational agent with web searching from LangChain.\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skog/Documents/code/ai/langchain-fun/langchain-tutorials/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:165: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/skog/Documents/code/ai/langchain-fun/langchain-tutorials/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:677: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the latest and most powerful model', 'answer': \"I don't know the latest and most powerful model as the provided content does not specify the model or the domain it belongs to.\\nSOURCES:\", 'sources': ''}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the latest and most powerful model\"\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4\", verbose=True)\n",
    "\n",
    "result = index.query_with_sources(query, llm=llm)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# print(result[\"answer\"])\n",
    "# print(result[\"sources\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
